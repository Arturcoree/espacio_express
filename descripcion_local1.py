# -*- coding: utf-8 -*-
"""descripcion_local1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-L3R0ptcmTQ0sxaPKe7Qw-x8_hRifkt2
"""

# Extraer Descripción del local de todos los anuncios

import pandas as pd
import requests
from bs4 import BeautifulSoup
import time

# Leer el archivo CSV
df = pd.read_csv(r'C:\Users\Jhonny\Documents\Hackathon\anuncios_actualizado16101845.csv')

# Definir los headers para la solicitud HTTP
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
}

# Crear una nueva columna para almacenar las descripciones de los locales
df['Descripción del local'] = ''

# Iterar sobre cada fila del DataFrame
for index, row in df.iterrows():
    # Obtener la URL de la fila
    url = row['URL']

    # Realizar la solicitud HTTP
    response = requests.get(url, headers=headers)

    # Verificar si la solicitud fue exitosa
    if response.status_code == 200:
        # Analizar el contenido HTML de la página
        soup = BeautifulSoup(response.text, 'html.parser')

        # Encontrar el elemento que contiene la descripción del local
        descripcion_element = soup.find('div', {'class': 'ant-typography property-description body body-regular body-1 high'})

        # Verificar si se encontró el elemento
        if descripcion_element:
            # Extraer el texto del elemento
            descripcion_text = descripcion_element.text

            # Agregar la descripción del local a la fila correspondiente
            df.at[index, 'Descripción del local'] = descripcion_text
        else:
            print(f"No se encontró la descripción del local en la URL: {url}")
    else:
        print(f"No se pudo acceder a la página web: {url}")

    # Esperar 2 segundos antes de realizar la siguiente solicitud
    time.sleep(2)

# Guardar el DataFrame en un nuevo archivo CSV
df.to_csv(r'C:\Users\Jhonny\Documents\Hackathon\anuncios_con_descripciones17102010.csv', index=False)