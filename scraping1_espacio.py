# -*- coding: utf-8 -*-
"""scraping1_espacio.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/151sO-4i6OpD7zSp5TjY3FBH0i2W9XaPm
"""

# Exportar a CSV scraping a todas las paginas de una web
import requests
import time
from bs4 import BeautifulSoup
import csv

base_url = 'https://www.fincaraiz.com.co/arriendo/locales/bogota/bogota-dc'
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}

# Realiza una solicitud HTTP a la página web
response = requests.get(base_url, headers=headers)

# Analiza el contenido HTML de la página web
soup = BeautifulSoup(response.content, 'html.parser')

# Encuentra el elemento 'ul' con la clase 'list' y el atributo 'role' igual a 'navigation'
pagination_ul = soup.find('ul', {'class': 'list', 'role': 'navigation'})

# Encuentra todos los elementos 'li' dentro del 'ul'
pagination_lis = pagination_ul.find_all('li')

# El penúltimo 'li' contiene el número de páginas, así que obtenemos su texto
num_pages = int(pagination_lis[-2].text)

# Crear una lista vacía para almacenar los datos de cada anuncio
data = []

# Itera sobre cada página y extrae los datos
for i in range(1, num_pages + 1):
    if i == 1:
        url = base_url
    else:
        url = base_url + "/pagina" + str(i)

    # Realiza una solicitud HTTP a la página actual
    response = requests.get(url, headers=headers)

    # Analiza el contenido HTML de la página actual
    soup = BeautifulSoup(response.content, 'html.parser')

    # Encuentra todos los elementos <div> con la clase listingCard
    locales = soup.find_all('div', {'class': 'listingCard'})

    # Itera sobre cada elemento <div> y extrae los datos
    for local in locales:
        titulo = local.find('span', {'class': 'lc-title'}).text
        precio = local.find('div', {'class': 'lc-price'}).find('span', {'class': 'heading heading-4 high'}).text
        precio = precio.replace('$', '').replace(',', '.')  # Quita el signo de pesos y reemplaza las comas por puntos
        ubicacion = local.find('strong', {'class': 'lc-location'}).text
        metros_cuadrados_elements = local.find('div', {'class': 'lc-typologyTag'}).find_all('strong')
        metros_cuadrados = ''
        if metros_cuadrados_elements:
            metros_cuadrados = metros_cuadrados_elements[-1].text
            metros_cuadrados = metros_cuadrados.replace('²', '2')
        url = 'https://www.fincaraiz.com.co' + local.find('a')['href']

        # Agregar los datos del anuncio a la lista
        data.append([titulo, precio, ubicacion, metros_cuadrados, url])

    # Agrega un tiempo de espera de 2 segundos entre las solicitudes
    time.sleep(2)

# Fuera del bucle, escribir los datos en un archivo CSV
with open('anuncios.csv', 'w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(['Titulo', 'Precio', 'Ubicacion', 'Metros cuadrados', 'URL'])
    writer.writerows(data)